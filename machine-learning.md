#   Machine Learning Modules (2 months)

##  Outline
| S/N | Domain      | Estimated Duration |
| --- | ----------- | ------------------ |
| 1   | Intro to ML | 1 week             |
| 2   | Practical   | 1 week             |
| 3   | Advanced ML | 4 weeks            |
| 4   | Assessment  | 2 weeks            |


##  Intro to ML
This section caters to officers without ML knowledge. Officers with a certain level of ML knowledge and experience can move straight to Advanced ML.

### Overview
*   [A visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)
*   [Data Science Primer by EliteDataScience](https://elitedatascience.com/primer)

### Lectures
*   [Machine Learning Algorithms (videos 1 to 6)](https://www.youtube.com/playlist?list=PLEiEAq2VkUULNa6MHQAZSOBxzB6HHFXj4)

### Tutorials
*   [Exploratory data analysis with Pandas](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic01_pandas_data_analysis/topic1_pandas_data_analysis.ipynb?flush_cache=true)
*   [Visual data analysis in Python](https://nbviewer.jupyter.org/github/Yorko/mlcourse_open/blob/master/jupyter_english/topic02_visual_data_analysis/topic2_visual_data_analysis.ipynb?flush_cache=true)


##  Practical
This section requires officers to go through the Kaggle Titanic dataset and understand the codes in the guided explanation. 

### Kaggle Titanic: Machine Learning from Disaster
*   https://www.kaggle.com/c/titanic

### Tips, tools, and guided explanation
*   [How to score 0.8134 in Titanic Kaggle Challenge by Ahmed Besbes](https://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html)
*   You can find the backup PDF of the guided explanation [here](ahmedbesbes-titanic.pdf)


##  Advanced ML

### Lectures
*   [IBM Machine Learning with Python (weeks 1 to 5)](https://www.coursera.org/learn/machine-learning-with-python)
*   [Stanford Machine Learning (weeks 3, 6, and 8)](https://www.coursera.org/learn/machine-learning)

### Overview
*   [Machine Learning for Everyone](https://vas3k.com/blog/machine_learning/)
*   [Model Tuning and the Bias-Variance Tradeoff](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)

### Ensemble Methods
*   [Kaggle Ensembling Guide](https://mlwave.com/kaggle-ensembling-guide/)
*   [Bagging, boosting and stacking in machine learning](https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning)

### Boosting
*   [Introduction to Boosted Trees](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
*   [Gradient Boosting Interactive Playground](http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)
*   [Gradient Boosting explained [demonstration]](http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html)

### Practical Application Issues
*   [Machine Learning Explainability](https://www.kaggle.com/learn/machine-learning-explainability)
*   [A Few Useful Things to Know about Machine Learning (section 2)](cacm12.pdf) (see also the image below)
    *   Symbolists = knowledge-based systems
    *   Connectionists = neural nets
    *   Evolutionaries = genetic algorithms *(not as relevant to data science)*
    *   Bayesians = bayes nets / expert systems
    *   Analogizers = support vector machines
    ![Representation, Evaluation, and Optimization for 5 approaches to ML](reo.png)

### Optimising thresholds
*   [Machine Learning Meets Economics](http://blog.mldb.ai/blog/posts/2016/01/ml-meets-economics/)
*   [Machine Learning Meets Economics, Part 2](http://blog.mldb.ai/blog/posts/2016/04/ml-meets-economics2/)

### Tutorials
*   [An introduction to machine learning with scikit-learn](https://scikit-learn.org/stable/tutorial/basic/tutorial.html)
*   [Supervised learning: predicting an output variable from high-dimensional observations](https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)
*   [Choosing the right estimator *(just for info)*](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)

### Available Toolkits
*   Common tools
    *   scikit-learn
    *   XGBoost
    *   Vowpal Wabbit
*   Obscure tools
    *   LightGBM / CatBoost
    *   Regularized Greedy Forest
    *   libFM / fastFM
    *   H2O

### Deep Learning Appreciation *(optional)*
*   [MIT Deep Learning (videos 1 and 2)](https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)
*   someday weâ€™ll add real deep learning with a GPU
    *   probably use MNIST or build word vectors with a small dataset


##  Assessment
This section is a hands-on assessment that requires officers to attempt the Kaggle Titanic prediction competition. Officers can either improve on the codes in the guided explanation, or write their own codes from scratch. Consequently, officers have to present their codes to the GL.
