#   Machine Learning

##  0: Contents
| S/N | Domain      | Estimated Duration |
| --- | ----------- | ------------------ |
| 1   | Intro to ML | 1 week             |
| 2   | Practical   | 1 week             |
| 3   | Advanced ML | 3 weeks            |
| 4   | Assessment  | 2 weeks            |



##  1: Intro to ML (1 week)

### Overview
*   http://www.r2d3.us/visual-intro-to-machine-learning-part-1/
*   https://elitedatascience.com/primer


### ML algorithms (videos 1 to 6)
*   https://www.youtube.com/playlist?list=PLEiEAq2VkUULNa6MHQAZSOBxzB6HHFXj4


### Data exploration and visualisation (topics 1 and 2)
*   https://mlcourse.ai/



##  2: Practical (1 week)

### Kaggle Titanic
*   https://www.kaggle.com/c/titanic


### Tips, tools, and guided explanation (PDF below)
*   https://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html
*   [backup pdf](ahmedbesbes-titanic.pdf)



##  3: Advanced Machine Learning (3 weeks)

### IBM Machine Learning with Python (weeks 1 to 5)
*   https://www.coursera.org/learn/machine-learning-with-python


### Stanford Machine Learning (weeks 3, 6, and 8)
*   https://www.coursera.org/learn/machine-learning


### Deep learning appreciation (optional; videos 1 and 2)
*   https://www.youtube.com/playlist?list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf


### Overview
*   https://vas3k.com/blog/machine_learning/
*   http://www.r2d3.us/visual-intro-to-machine-learning-part-2/


### Ensemble methods
*   https://mlwave.com/kaggle-ensembling-guide/
*   https://stats.stackexchange.com/questions/18891/bagging-boosting-and-stacking-in-machine-learning


### Boosting
*   https://xgboost.readthedocs.io/en/latest/tutorials/model.html
*   http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html
*   http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html


### ML explainability
*   https://www.kaggle.com/learn/machine-learning-explainability


### ML = Representation + Evaluation + Optimization (section 2)
*   [A Few Useful Things to Know about Machine Learning](cacm12.pdf)
*   see also this image:
    ![Representation, Evaluation, and Optimization for 5 approaches to ML](reo.png)
    *   Symbolists = knowledge-based systems
    *   Connectionists = neural nets
    *   Evolutionaries = genetic algorithms *(not as relevant to data science)*
    *   Bayesians = bayes nets / expert systems
    *   Analogizers = support vector machines


### Optimising thresholds
*   http://blog.mldb.ai/blog/posts/2016/01/ml-meets-economics/
*   http://blog.mldb.ai/blog/posts/2016/04/ml-meets-economics2/


### Introduction to ML with scikit-learn
*   https://scikit-learn.org/stable/tutorial/basic/tutorial.html


### Supervised learning with scikit-learn
*   https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html


### Choosing the right estimator *(just for info)*
*   https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html


### Available Toolkits *(just for info)*
*   Common tools
    *   scikit-learn
    *   XGBoost
    *   Vowpal Wabbit
*   Obscure tools
    *   LightGBM / CatBoost
    *   Regularized Greedy Forest
    *   libFM / fastFM
    *   H2O


### Deep Learning (TODO)
*   someday weâ€™ll add something using a GPU
*   probably use MNIST or build word vectors with a small dataset



##  4: Assessment (2 weeks)
*   Officer to improve on the Kaggle Titanic codes given in the guided explanation
